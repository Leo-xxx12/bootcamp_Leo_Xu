{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leo-xxx12/bootcamp_Leo_Xu/blob/main/Copy_of_stage06_data_preprocessing_homework_starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-rXOUeqn50h"
      },
      "source": [
        "# Setup: Generate Sample Dataset\n",
        "\n",
        "This cell creates the required folder structure (`data/raw/` and `data/processed/`) relative to the notebook, and generates the sample CSV dataset with missing values.\n",
        "This ensures the dataset is ready for cleaning functions and saves it to `data/raw/sample_data.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Rfl-euzTn50l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5538ef78-6d11-4322-f75d-b48f21c6a7c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample dataset created and saved to ../data/raw/sample_data.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define folder paths relative to this notebook\n",
        "raw_dir = '../data/raw'\n",
        "processed_dir = '../data/processed'\n",
        "\n",
        "# Create folders if they don't exist\n",
        "os.makedirs(raw_dir, exist_ok=True)\n",
        "os.makedirs(processed_dir, exist_ok=True)\n",
        "\n",
        "# Define the sample data\n",
        "data = {\n",
        "    'age': [34, 45, 29, 50, 38, np.nan, 41],\n",
        "    'income': [55000, np.nan, 42000, 58000, np.nan, np.nan, 49000],\n",
        "    'score': [0.82, 0.91, np.nan, 0.76, 0.88, 0.65, 0.79],\n",
        "    'zipcode': ['90210', '10001', '60614', '94103', '73301', '12345', '94105'],\n",
        "    'city': ['Beverly', 'New York', 'Chicago', 'SF', 'Austin', 'Unknown', 'San Francisco'],\n",
        "    'extra_data': [np.nan, 42, np.nan, np.nan, np.nan, 5, np.nan]\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save to CSV in raw data folder\n",
        "csv_path = os.path.join(raw_dir, 'sample_data.csv')\n",
        "if not os.path.exists(csv_path):\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f'Sample dataset created and saved to {csv_path}')\n",
        "else:\n",
        "    print(f'File already exists at {csv_path}. Skipping CSV creation to avoid overwrite.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG-0V2wen50m"
      },
      "source": [
        "# Homework Starter — Stage 6: Data Preprocessing\n",
        "Use this notebook to apply your cleaning functions and save processed data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "k-uoCP8Bn50n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "de14eeaf-ab62-4f6a-8fcf-6ca47ec2e2a5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'src'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-611519085.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcleaning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def fill_missing_median(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Fill numeric NaNs with column medians (returns a new DataFrame).\"\"\"\n",
        "    out = df.copy()\n",
        "    med = out.median(numeric_only=True)\n",
        "    out[med.index] = out[med.index].fillna(med)\n",
        "    return out\n",
        "\n",
        "def drop_missing(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Drop rows that still contain any NaNs.\"\"\"\n",
        "    return df.dropna().reset_index(drop=True)\n",
        "\n",
        "def normalize_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Min-max scale numeric columns to [0,1]; leave non-numeric unchanged.\"\"\"\n",
        "    out = df.copy()\n",
        "    num_cols = out.select_dtypes(include=\"number\").columns\n",
        "    if len(num_cols) == 0:\n",
        "        return out\n",
        "    scaler = MinMaxScaler()\n",
        "    out[num_cols] = scaler.fit_transform(out[num_cols])\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFsPPizon50n"
      },
      "source": [
        "## Load Raw Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, datetime as dt\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# load env\n",
        "load_dotenv()\n",
        "RAW = Path(os.getenv(\"DATA_DIR_RAW\", \"project/data/raw\"))\n",
        "PROC = Path(os.getenv(\"DATA_DIR_PROCESSED\", \"project/data/processed\"))\n",
        "RAW.mkdir(parents=True, exist_ok=True)\n",
        "PROC.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "from src.cleaning import fill_missing_median, drop_missing, normalize_data\n",
        "\n",
        "def ts(): return dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n"
      ],
      "metadata": {
        "id": "1lG4BmsuopfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_files = sorted(glob.glob(str(RAW / \"*.csv\")))\n",
        "assert raw_files, \"No CSV files found in data/raw/\"\n",
        "raw_path = Path(raw_files[0])\n",
        "\n",
        "df_raw = pd.read_csv(raw_path)  # add parse_dates=['date'] if you have dates\n",
        "df_raw.head()\n"
      ],
      "metadata": {
        "id": "CvsB2D6hoqZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = df_raw.pipe(fill_missing_median).pipe(drop_missing).pipe(normalize_data)\n",
        "df_clean.head()\n"
      ],
      "metadata": {
        "id": "n4TgZBYeovxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_pq = PROC / f\"cleaned_{ts()}.parquet\"\n",
        "try:\n",
        "    df_clean.to_parquet(out_pq, index=False)\n",
        "    print(f\"Saved cleaned parquet → {out_pq}\")\n",
        "except Exception as e:\n",
        "    print(\"Parquet engine missing. Install one of:\\n  pip install pyarrow\\n  # or\\n  pip install fastparquet\")\n",
        "    raise\n"
      ],
      "metadata": {
        "id": "ofhrEKSKoyEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LmbxfJnn50n"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('../data/raw/sample_data.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary = pd.DataFrame({\n",
        "    \"metric\": [\n",
        "        \"rows\", \"columns\",\n",
        "        \"rows_after_clean\",\n",
        "        \"num_cols_raw\", \"num_cols_clean\",\n",
        "        \"cols_with_any_na_raw\", \"cols_with_any_na_clean\",\n",
        "    ],\n",
        "    \"value\": [\n",
        "        len(df_raw), df_raw.shape[1],\n",
        "        len(df_clean),\n",
        "        df_raw.select_dtypes(\"number\").shape[1],\n",
        "        df_clean.select_dtypes(\"number\").shape[1],\n",
        "        df_raw.isna().any().sum(),\n",
        "        df_clean.isna().any().sum(),\n",
        "    ],\n",
        "})\n",
        "summary\n"
      ],
      "metadata": {
        "id": "vHSaaYKMo4Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84sjYSkgn50o"
      },
      "source": [
        "## Apply Cleaning Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0wbz7y-n50o"
      },
      "outputs": [],
      "source": [
        "# TODO: Apply your functions here\n",
        "# Example:\n",
        "# df = cleaning.fill_missing_median(df, ['col1','col2'])\n",
        "# df = cleaning.drop_missing(df, threshold=0.5)\n",
        "# df = cleaning.normalize_data(df, ['col1','col2'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWXtpbNAn50o"
      },
      "source": [
        "## Save Cleaned Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwJMTodJn50o"
      },
      "outputs": [],
      "source": [
        "# df.to_csv('../data/processed/sample_data_cleaned.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
