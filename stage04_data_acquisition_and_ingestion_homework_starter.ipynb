{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leo-xxx12/bootcamp_Leo_Xu/blob/main/stage04_data_acquisition_and_ingestion_homework_starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EvwjMl50o7x"
      },
      "source": [
        "# Homework Starter — Stage 04: Data Acquisition and Ingestion\n",
        "Name:\n",
        "Date:\n",
        "\n",
        "## Objectives\n",
        "- API ingestion with secrets in `.env`\n",
        "- Scrape a permitted public table\n",
        "- Validate and save raw data to `data/raw/`"
      ],
      "id": "7EvwjMl50o7x"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Aba9RQVX0o71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8da95c32-d283-40b3-c9aa-cfd76977b1e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ALPHAVANTAGE_API_KEY loaded? False\n"
          ]
        }
      ],
      "source": [
        "import os, pathlib, datetime as dt\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "RAW = pathlib.Path('data/raw'); RAW.mkdir(parents=True, exist_ok=True)\n",
        "load_dotenv(); print('ALPHAVANTAGE_API_KEY loaded?', bool(os.getenv('ALPHAVANTAGE_API_KEY')))"
      ],
      "id": "Aba9RQVX0o71"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_IAsu-G0o73"
      },
      "source": [
        "## Helpers (use or modify)"
      ],
      "id": "O_IAsu-G0o73"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zhabYKRu0o74"
      },
      "outputs": [],
      "source": [
        "def ts():\n",
        "    return dt.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "\n",
        "def save_csv(df: pd.DataFrame, prefix: str, **meta):\n",
        "    mid = '_'.join([f\"{k}-{v}\" for k,v in meta.items()])\n",
        "    path = RAW / f\"{prefix}_{mid}_{ts()}.csv\"\n",
        "    df.to_csv(path, index=False)\n",
        "    print('Saved', path)\n",
        "    return path\n",
        "\n",
        "def validate(df: pd.DataFrame, required):\n",
        "    missing = [c for c in required if c not in df.columns]\n",
        "    return {'missing': missing, 'shape': df.shape, 'na_total': int(df.isna().sum().sum())}"
      ],
      "id": "zhabYKRu0o74"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpIz2X2w0o74"
      },
      "source": [
        "## Part 1 — API Pull (Required)\n",
        "Choose an endpoint (e.g., Alpha Vantage or use `yfinance` fallback)."
      ],
      "id": "CpIz2X2w0o74"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2AWoJwgR0o74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "8c6783e6-2cd9-4e7a-d1a1-ae723fb79db1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2430833282.py:14: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df_api = yf.download(SYMBOL, period='3mo', interval='1d').reset_index()[['Date','Adj Close']]\n",
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['Adj Close'] not in index\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2430833282.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0myfinance\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0myf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdf_api\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSYMBOL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'3mo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Adj Close'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mdf_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'adj_close'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   2761\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_level_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2763\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2764\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   2779\u001b[0m                 \u001b[0mcmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2780\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2781\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{keyarr[cmask]} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2782\u001b[0m                 \u001b[0;31m# We get here when levels still contain values which are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2783\u001b[0m                 \u001b[0;31m# actually in Index anymore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['Adj Close'] not in index\""
          ]
        }
      ],
      "source": [
        "SYMBOL = 'AAPL'\n",
        "USE_ALPHA = bool(os.getenv('ALPHAVANTAGE_API_KEY'))\n",
        "if USE_ALPHA:\n",
        "    url = 'https://www.alphavantage.co/query'\n",
        "    params = {'function':'TIME_SERIES_DAILY_ADJUSTED','symbol':SYMBOL,'outputsize':'compact','apikey':os.getenv('ALPHAVANTAGE_API_KEY')}\n",
        "    r = requests.get(url, params=params, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    js = r.json()\n",
        "    key = [k for k in js if 'Time Series' in k][0]\n",
        "    df_api = pd.DataFrame(js[key]).T.reset_index().rename(columns={'index':'date','5. adjusted close':'adj_close'})[['date','adj_close']]\n",
        "    df_api['date'] = pd.to_datetime(df_api['date']); df_api['adj_close'] = pd.to_numeric(df_api['adj_close'])\n",
        "else:\n",
        "    import yfinance as yf\n",
        "    df_api = yf.download(SYMBOL, period='3mo', interval='1d').reset_index()[['Date','Adj Close']]\n",
        "    df_api.columns = ['date','adj_close']\n",
        "\n",
        "v_api = validate(df_api, ['date','adj_close']); v_api"
      ],
      "id": "2AWoJwgR0o74"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7mxJVAS0o75"
      },
      "outputs": [],
      "source": [
        "_ = save_csv(df_api.sort_values('date'), prefix='api', source='alpha' if USE_ALPHA else 'yfinance', symbol=SYMBOL)"
      ],
      "id": "F7mxJVAS0o75"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import os\n",
        "from datetime import datetime\n",
        "import yfinance as yf\n",
        "\n",
        "# Configuration\n",
        "SYMBOL = 'AAPL'\n",
        "\n",
        "# Load API key from .env (if required)\n",
        "def load_api_key():\n",
        "    \"\"\"Load API key from environment or .env file\"\"\"\n",
        "    try:\n",
        "        # Try to get from environment first\n",
        "        api_key = os.getenv('ALPHAVANTAGE_API_KEY')\n",
        "        if api_key:\n",
        "            return api_key\n",
        "\n",
        "        # If not in environment, try to load from .env file\n",
        "        try:\n",
        "            with open('.env', 'r') as f:\n",
        "                for line in f:\n",
        "                    if line.startswith('ALPHAVANTAGE_API_KEY='):\n",
        "                        return line.split('=', 1)[1].strip()\n",
        "        except FileNotFoundError:\n",
        "            print(\"No .env file found\")\n",
        "\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading API key: {e}\")\n",
        "        return None\n",
        "\n",
        "# Choose ticker/endpoint and determine data source\n",
        "def get_stock_data(symbol):\n",
        "    \"\"\"\n",
        "    Choose one ticker or endpoint.\n",
        "    Request data with requests (or yfinance as fallback).\n",
        "    \"\"\"\n",
        "\n",
        "    # Load API key from .env (if required)\n",
        "    api_key = load_api_key()\n",
        "    use_alpha = bool(api_key)\n",
        "\n",
        "    if use_alpha:\n",
        "        print(f\"Using Alpha Vantage API for {symbol}\")\n",
        "        try:\n",
        "            # Request data with requests\n",
        "            url = 'https://www.alphavantage.co/query'\n",
        "            params = {\n",
        "                'function': 'TIME_SERIES_DAILY_ADJUSTED',\n",
        "                'symbol': symbol,\n",
        "                'outputsize': 'compact',\n",
        "                'apikey': api_key\n",
        "            }\n",
        "\n",
        "            r = requests.get(url, params=params, timeout=30)\n",
        "            r.raise_for_status()\n",
        "            js = r.json()\n",
        "\n",
        "            # Check for API errors\n",
        "            if 'Error Message' in js:\n",
        "                raise Exception(f\"API Error: {js['Error Message']}\")\n",
        "            if 'Note' in js:\n",
        "                raise Exception(f\"API Limit: {js['Note']}\")\n",
        "\n",
        "            # Convert to DataFrame; parse dtypes (dates, floats)\n",
        "            key = [k for k in js if 'Time Series' in k][0]\n",
        "            df_api = pd.DataFrame(js[key]).T.reset_index()\n",
        "            df_api = df_api.rename(columns={\n",
        "                'index': 'date',\n",
        "                '5. adjusted close': 'adj_close'\n",
        "            })\n",
        "\n",
        "            # Parse dtypes (dates, floats)\n",
        "            df_api['date'] = pd.to_datetime(df_api['date'])\n",
        "            numeric_cols = [col for col in df_api.columns if col != 'date']\n",
        "            for col in numeric_cols:\n",
        "                df_api[col] = pd.to_numeric(df_api[col], errors='coerce')\n",
        "\n",
        "            return df_api, 'alpha'\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Alpha Vantage failed: {e}, falling back to yfinance\")\n",
        "            use_alpha = False\n",
        "\n",
        "    if not use_alpha:\n",
        "        print(f\"Using yfinance fallback for {symbol}\")\n",
        "        # yfinance as fallback\n",
        "        df_api = yf.download(symbol, period='3mo', interval='1d').reset_index()\n",
        "\n",
        "        # Handle column naming more robustly\n",
        "        print(f\"Original columns: {list(df_api.columns)}\")\n",
        "        print(f\"Shape: {df_api.shape}\")\n",
        "\n",
        "        # Convert multi-level columns to single level if needed\n",
        "        if isinstance(df_api.columns, pd.MultiIndex):\n",
        "            df_api.columns = [col[0] if col[0] != symbol else col[1] for col in df_api.columns]\n",
        "\n",
        "        # Map existing columns to standardized names\n",
        "        column_mapping = {}\n",
        "        current_cols = list(df_api.columns)\n",
        "\n",
        "        for col in current_cols:\n",
        "            col_lower = str(col).lower()\n",
        "            if 'date' in col_lower or col == 'Date':\n",
        "                column_mapping[col] = 'date'\n",
        "            elif 'open' in col_lower:\n",
        "                column_mapping[col] = 'open'\n",
        "            elif 'high' in col_lower:\n",
        "                column_mapping[col] = 'high'\n",
        "            elif 'low' in col_lower:\n",
        "                column_mapping[col] = 'low'\n",
        "            elif 'close' in col_lower and 'adj' not in col_lower:\n",
        "                column_mapping[col] = 'close'\n",
        "            elif 'adj' in col_lower and 'close' in col_lower:\n",
        "                column_mapping[col] = 'adj_close'\n",
        "            elif 'volume' in col_lower:\n",
        "                column_mapping[col] = 'volume'\n",
        "\n",
        "        df_api = df_api.rename(columns=column_mapping)\n",
        "\n",
        "        # Ensure we have the required columns\n",
        "        if 'adj_close' not in df_api.columns and 'close' in df_api.columns:\n",
        "            df_api['adj_close'] = df_api['close']\n",
        "\n",
        "        # Parse dtypes (dates, floats) - yfinance usually handles this automatically\n",
        "        if 'date' in df_api.columns:\n",
        "            df_api['date'] = pd.to_datetime(df_api['date'])\n",
        "\n",
        "        print(f\"Final columns: {list(df_api.columns)}\")\n",
        "\n",
        "        return df_api, 'yfinance'\n",
        "\n",
        "def validate_dataframe(df, required_columns=None):\n",
        "    \"\"\"\n",
        "    Validate (required columns, NA counts, shape).\n",
        "    \"\"\"\n",
        "    if required_columns is None:\n",
        "        required_columns = ['date', 'adj_close']\n",
        "\n",
        "    validation_results = {\n",
        "        'shape': df.shape,\n",
        "        'required_columns_present': all(col in df.columns for col in required_columns),\n",
        "        'missing_columns': [col for col in required_columns if col not in df.columns],\n",
        "        'na_counts': df.isnull().sum().to_dict(),\n",
        "        'total_na': df.isnull().sum().sum(),\n",
        "        'date_range': None,\n",
        "        'valid': True,\n",
        "        'issues': []\n",
        "    }\n",
        "\n",
        "    # Check required columns\n",
        "    if not validation_results['required_columns_present']:\n",
        "        validation_results['valid'] = False\n",
        "        validation_results['issues'].append(f\"Missing required columns: {validation_results['missing_columns']}\")\n",
        "\n",
        "    # Check shape\n",
        "    if df.shape[0] == 0:\n",
        "        validation_results['valid'] = False\n",
        "        validation_results['issues'].append(\"DataFrame is empty\")\n",
        "\n",
        "    # Check date range if date column exists\n",
        "    if 'date' in df.columns:\n",
        "        validation_results['date_range'] = {\n",
        "            'start': df['date'].min(),\n",
        "            'end': df['date'].max(),\n",
        "            'days': len(df)\n",
        "        }\n",
        "\n",
        "    # Check for excessive NAs\n",
        "    if validation_results['total_na'] > len(df) * 0.5:  # More than 50% NA\n",
        "        validation_results['issues'].append(\"Excessive missing values (>50%)\")\n",
        "\n",
        "    return validation_results\n",
        "\n",
        "def save_raw_csv(df, prefix='api', source='unknown', symbol=''):\n",
        "    \"\"\"\n",
        "    Save raw CSV to data/raw/.\n",
        "    \"\"\"\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs('data/raw', exist_ok=True)\n",
        "\n",
        "    # Generate filename with timestamp\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    filename = f\"{prefix}_{source}_{symbol}_{timestamp}.csv\"\n",
        "    filepath = os.path.join('data/raw', filename)\n",
        "\n",
        "    # Sort by date before saving\n",
        "    if 'date' in df.columns:\n",
        "        df = df.sort_values('date')\n",
        "\n",
        "    # Save to CSV\n",
        "    df.to_csv(filepath, index=False)\n",
        "    print(f\"Raw CSV saved to: {filepath}\")\n",
        "\n",
        "    return filepath\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to execute the API pull process\"\"\"\n",
        "    print(\"=== API Pull Process ===\")\n",
        "\n",
        "    # Get stock data\n",
        "    df_api, source = get_stock_data(SYMBOL)\n",
        "\n",
        "    # Validate (required columns, NA counts, shape)\n",
        "    validation_results = validate_dataframe(df_api, ['date', 'adj_close'])\n",
        "\n",
        "    print(f\"\\n=== Validation Results ===\")\n",
        "    print(f\"Shape: {validation_results['shape']}\")\n",
        "    print(f\"Required columns present: {validation_results['required_columns_present']}\")\n",
        "    print(f\"Total NA values: {validation_results['total_na']}\")\n",
        "    print(f\"Date range: {validation_results['date_range']}\")\n",
        "    print(f\"Valid: {validation_results['valid']}\")\n",
        "\n",
        "    if validation_results['issues']:\n",
        "        print(\"Issues found:\")\n",
        "        for issue in validation_results['issues']:\n",
        "            print(f\"  - {issue}\")\n",
        "\n",
        "    if not validation_results['valid']:\n",
        "        print(\"Validation failed! Please check the data.\")\n",
        "        return None\n",
        "\n",
        "    # Save raw CSV to data/raw/\n",
        "    filepath = save_raw_csv(df_api, prefix='api', source=source, symbol=SYMBOL)\n",
        "\n",
        "    print(f\"\\n=== Process Complete ===\")\n",
        "    print(f\"Data source: {source}\")\n",
        "    print(f\"Records: {len(df_api)}\")\n",
        "    print(f\"File saved: {filepath}\")\n",
        "\n",
        "    return df_api, validation_results\n",
        "\n",
        "# Execute if run directly\n",
        "if __name__ == \"__main__\":\n",
        "    result = main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZeRXIZl4rcs",
        "outputId": "a683e0db-ffa1-4b38-841d-28da76d829e0"
      },
      "id": "-ZeRXIZl4rcs",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3148973483.py:89: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df_api = yf.download(symbol, period='3mo', interval='1d').reset_index()\n",
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== API Pull Process ===\n",
            "No .env file found\n",
            "Using yfinance fallback for AAPL\n",
            "Original columns: [('Date', ''), ('Close', 'AAPL'), ('High', 'AAPL'), ('Low', 'AAPL'), ('Open', 'AAPL'), ('Volume', 'AAPL')]\n",
            "Shape: (64, 6)\n",
            "Final columns: ['date', 'close', 'high', 'low', 'open', 'volume', 'adj_close']\n",
            "\n",
            "=== Validation Results ===\n",
            "Shape: (64, 7)\n",
            "Required columns present: True\n",
            "Total NA values: 0\n",
            "Date range: {'start': Timestamp('2025-05-19 00:00:00'), 'end': Timestamp('2025-08-19 00:00:00'), 'days': 64}\n",
            "Valid: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw CSV saved to: data/raw/api_yfinance_AAPL_20250819_152713.csv\n",
            "\n",
            "=== Process Complete ===\n",
            "Data source: yfinance\n",
            "Records: 64\n",
            "File saved: data/raw/api_yfinance_AAPL_20250819_152713.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEWVoaYS0o75"
      },
      "source": [
        "## Part 2 — Scrape a Public Table (Required)\n",
        "Replace `SCRAPE_URL` with a permitted page containing a simple table."
      ],
      "id": "UEWVoaYS0o75"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Can8-qxR0o75"
      },
      "outputs": [],
      "source": [
        "SCRAPE_URL = 'https://example.com/markets-table'  # TODO: replace with permitted page\n",
        "headers = {'User-Agent':'AFE-Homework/1.0'}\n",
        "try:\n",
        "    resp = requests.get(SCRAPE_URL, headers=headers, timeout=30); resp.raise_for_status()\n",
        "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "    rows = [[c.get_text(strip=True) for c in tr.find_all(['th','td'])] for tr in soup.find_all('tr')]\n",
        "    header, *data = [r for r in rows if r]\n",
        "    df_scrape = pd.DataFrame(data, columns=header)\n",
        "except Exception as e:\n",
        "    print('Scrape failed, using inline demo table:', e)\n",
        "    html = '<table><tr><th>Ticker</th><th>Price</th></tr><tr><td>AAA</td><td>101.2</td></tr></table>'\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    rows = [[c.get_text(strip=True) for c in tr.find_all(['th','td'])] for tr in soup.find_all('tr')]\n",
        "    header, *data = [r for r in rows if r]\n",
        "    df_scrape = pd.DataFrame(data, columns=header)\n",
        "\n",
        "if 'Price' in df_scrape.columns:\n",
        "    df_scrape['Price'] = pd.to_numeric(df_scrape['Price'], errors='coerce')\n",
        "v_scrape = validate(df_scrape, list(df_scrape.columns)); v_scrape"
      ],
      "id": "Can8-qxR0o75"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81d5be--0o76"
      },
      "outputs": [],
      "source": [
        "_ = save_csv(df_scrape, prefix='scrape', site='example', table='markets')"
      ],
      "id": "81d5be--0o76"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import os\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def scrape_public_table(scrape_url=None):\n",
        "    \"\"\"\n",
        "    Scrape a Small Table (required):\n",
        "    - Public, permitted page with a simple table\n",
        "    - Parse with BeautifulSoup; build DataFrame\n",
        "    - Validate numeric/text columns\n",
        "    - Save raw CSV to data/raw/\n",
        "    \"\"\"\n",
        "\n",
        "    # Default to a public table if no URL provided\n",
        "    if scrape_url is None:\n",
        "        # Example: Wikipedia table (public, permitted)\n",
        "        scrape_url = 'https://en.wikipedia.org/wiki/List_of_largest_companies_by_revenue'\n",
        "\n",
        "    print(f\"Scraping table from: {scrape_url}\")\n",
        "\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
        "\n",
        "    try:\n",
        "        # Request the page\n",
        "        resp = requests.get(scrape_url, headers=headers, timeout=30)\n",
        "        resp.raise_for_status()\n",
        "\n",
        "        # Parse with BeautifulSoup; build DataFrame\n",
        "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "\n",
        "        # Find the first table with data\n",
        "        tables = soup.find_all('table')\n",
        "\n",
        "        if not tables:\n",
        "            raise Exception(\"No tables found on the page\")\n",
        "\n",
        "        # Try to find a table with meaningful data\n",
        "        df_scrape = None\n",
        "        for table in tables:\n",
        "            rows = [[c.get_text(strip=True) for c in tr.find_all(['th', 'td'])] for tr in table.find_all('tr')]\n",
        "\n",
        "            # Filter out empty rows\n",
        "            rows = [r for r in rows if r and any(cell.strip() for cell in r)]\n",
        "\n",
        "            if len(rows) > 2:  # At least header + 2 data rows\n",
        "                # Use first row as header\n",
        "                header = rows[0]\n",
        "                data = rows[1:]\n",
        "\n",
        "                # Ensure all rows have same number of columns\n",
        "                max_cols = len(header)\n",
        "                data = [row[:max_cols] + [''] * (max_cols - len(row)) for row in data]\n",
        "\n",
        "                df_scrape = pd.DataFrame(data, columns=header)\n",
        "                break\n",
        "\n",
        "        if df_scrape is None or df_scrape.empty:\n",
        "            raise Exception(\"Could not extract meaningful table data\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Scrape failed, using demo table: {e}')\n",
        "\n",
        "        # Fallback demo table\n",
        "        html = '<table><tr><th>Company</th><th>Revenue</th><th>Country</th></tr><tr><td>Walmart</td><td>611.3</td><td>USA</td></tr><tr><td>Amazon</td><td>513.98</td><td>USA</td></tr><tr><td>Apple</td><td>394.33</td><td>USA</td></tr></table>'\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        rows = [[c.get_text(strip=True) for c in tr.find_all(['th', 'td'])] for tr in soup.find_all('tr')]\n",
        "        header, *data = [r for r in rows if r]\n",
        "        df_scrape = pd.DataFrame(data, columns=header)\n",
        "\n",
        "    print(f\"Scraped table shape: {df_scrape.shape}\")\n",
        "    print(f\"Columns: {list(df_scrape.columns)}\")\n",
        "\n",
        "    return df_scrape\n",
        "\n",
        "def validate_scraped_data(df):\n",
        "    \"\"\"\n",
        "    Validate numeric/text columns for scraped data\n",
        "    \"\"\"\n",
        "    validation_results = {\n",
        "        'shape': df.shape,\n",
        "        'columns': list(df.columns),\n",
        "        'column_types': {},\n",
        "        'numeric_columns': [],\n",
        "        'text_columns': [],\n",
        "        'issues': [],\n",
        "        'valid': True\n",
        "    }\n",
        "\n",
        "    for col in df.columns:\n",
        "        # Try to convert to numeric\n",
        "        numeric_series = pd.to_numeric(df[col], errors='coerce')\n",
        "        numeric_count = numeric_series.notna().sum()\n",
        "        total_count = len(df[col])\n",
        "\n",
        "        # If more than 70% can be converted to numeric, treat as numeric\n",
        "        if numeric_count / total_count > 0.7:\n",
        "            validation_results['column_types'][col] = 'numeric'\n",
        "            validation_results['numeric_columns'].append(col)\n",
        "            # Update the column to numeric\n",
        "            df[col] = numeric_series\n",
        "        else:\n",
        "            validation_results['column_types'][col] = 'text'\n",
        "            validation_results['text_columns'].append(col)\n",
        "\n",
        "    # Check for issues\n",
        "    if df.shape[0] == 0:\n",
        "        validation_results['valid'] = False\n",
        "        validation_results['issues'].append(\"DataFrame is empty\")\n",
        "\n",
        "    if df.shape[1] == 0:\n",
        "        validation_results['valid'] = False\n",
        "        validation_results['issues'].append(\"No columns found\")\n",
        "\n",
        "    # Check for excessive missing data\n",
        "    missing_percentage = df.isnull().sum().sum() / (df.shape[0] * df.shape[1])\n",
        "    if missing_percentage > 0.5:\n",
        "        validation_results['issues'].append(f\"High missing data: {missing_percentage:.1%}\")\n",
        "\n",
        "    return validation_results\n",
        "\n",
        "def save_scraped_csv(df, prefix='scrape', site='unknown', table='table'):\n",
        "    \"\"\"\n",
        "    Save raw CSV to data/raw/ for scraped data\n",
        "    \"\"\"\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs('data/raw', exist_ok=True)\n",
        "\n",
        "    # Generate filename with timestamp\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    filename = f\"{prefix}_{site}_{table}_{timestamp}.csv\"\n",
        "    filepath = os.path.join('data/raw', filename)\n",
        "\n",
        "    # Save to CSV\n",
        "    df.to_csv(filepath, index=False)\n",
        "    print(f\"Scraped CSV saved to: {filepath}\")\n",
        "\n",
        "    return filepath\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to execute web scraping process\"\"\"\n",
        "    print(\"=== Web Scraping Process ===\")\n",
        "\n",
        "    # Scrape a public table\n",
        "    df_scrape = scrape_public_table()\n",
        "\n",
        "    # Validate numeric/text columns\n",
        "    scrape_validation = validate_scraped_data(df_scrape)\n",
        "\n",
        "    print(f\"\\n=== Validation Results ===\")\n",
        "    print(f\"Shape: {scrape_validation['shape']}\")\n",
        "    print(f\"Columns: {scrape_validation['columns']}\")\n",
        "    print(f\"Numeric columns: {scrape_validation['numeric_columns']}\")\n",
        "    print(f\"Text columns: {scrape_validation['text_columns']}\")\n",
        "    print(f\"Valid: {scrape_validation['valid']}\")\n",
        "\n",
        "    if scrape_validation['issues']:\n",
        "        print(\"Issues found:\")\n",
        "        for issue in scrape_validation['issues']:\n",
        "            print(f\"  - {issue}\")\n",
        "\n",
        "    # Save raw CSV to data/raw/\n",
        "    scrape_filepath = save_scraped_csv(df_scrape, prefix='scrape', site='wikipedia', table='companies')\n",
        "\n",
        "    print(f\"\\n=== Process Complete ===\")\n",
        "    print(f\"Records: {len(df_scrape)}\")\n",
        "    print(f\"File saved: {scrape_filepath}\")\n",
        "\n",
        "    return {\n",
        "        'scrape_data': df_scrape,\n",
        "        'scrape_validation': scrape_validation,\n",
        "        'scrape_file': scrape_filepath\n",
        "    }\n",
        "\n",
        "# Execute if run directly\n",
        "if __name__ == \"__main__\":\n",
        "    result = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEQvEXN69yOJ",
        "outputId": "01058bcd-ff77-4058-a299-f777b71a0a65"
      },
      "id": "BEQvEXN69yOJ",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Web Scraping Process ===\n",
            "Scraping table from: https://en.wikipedia.org/wiki/List_of_largest_companies_by_revenue\n",
            "Scraped table shape: (50, 9)\n",
            "Columns: ['Ranks', 'Name', 'Industry', 'Revenue', 'Profit', 'Employees', 'Headquarters[note 1]', 'State-owned', 'Ref.']\n",
            "\n",
            "=== Validation Results ===\n",
            "Shape: (50, 9)\n",
            "Columns: ['Ranks', 'Name', 'Industry', 'Revenue', 'Profit', 'Employees', 'Headquarters[note 1]', 'State-owned', 'Ref.']\n",
            "Numeric columns: ['Ranks']\n",
            "Text columns: ['Name', 'Industry', 'Revenue', 'Profit', 'Employees', 'Headquarters[note 1]', 'State-owned', 'Ref.']\n",
            "Valid: True\n",
            "Scraped CSV saved to: data/raw/scrape_wikipedia_companies_20250819_153359.csv\n",
            "\n",
            "=== Process Complete ===\n",
            "Records: 50\n",
            "File saved: data/raw/scrape_wikipedia_companies_20250819_153359.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mpc5Dbdy0o76"
      },
      "source": [
        "## Documentation\n",
        "- API Source: (URL/endpoint/params)\n",
        "- Scrape Source: (URL/table description)\n",
        "- Assumptions & risks: (rate limits, selector fragility, schema changes)\n",
        "- Confirm `.env` is not committed."
      ],
      "id": "Mpc5Dbdy0o76"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}