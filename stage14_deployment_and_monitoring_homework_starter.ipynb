{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leo-xxx12/bootcamp_Leo_Xu/blob/main/stage14_deployment_and_monitoring_homework_starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VHrpbSHG7fx"
      },
      "source": [
        "# Homework Starter — Stage 14: Deployment & Monitoring\n",
        "\n",
        "Use this template to draft your reflection and (optionally) sketch a dashboard."
      ],
      "id": "2VHrpbSHG7fx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0smvBW9G7fz"
      },
      "source": [
        "## 1) Reflection (200–300 words)\n",
        "- Risks if deployed:\n",
        "- Monitoring metrics across layers (Data/Model/System/Business):\n",
        "- Ownership & handoffs:\n",
        "\n",
        "> Tip: Be specific (e.g., 'p95 latency > 250ms triggers on-call notification')."
      ],
      "id": "O0smvBW9G7fz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXtJYoUPG7f0"
      },
      "source": [
        "## 2) Optional: Dashboard Sketch\n",
        "Describe panels and key charts. You can also attach an image file in your repo (png/pdf)."
      ],
      "id": "vXtJYoUPG7f0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VpnOFqhG7f0"
      },
      "outputs": [],
      "source": [
        "# Optional helper: simple structure to list metrics\n",
        "monitoring = {\n",
        "    'data': ['freshness_minutes', 'null_rate', 'schema_hash'],\n",
        "    'model': ['rolling_mae_or_auc', 'calibration_error'],\n",
        "    'system': ['p95_latency_ms', 'error_rate'],\n",
        "    'business': ['approval_rate', 'bad_rate']\n",
        "}\n",
        "monitoring"
      ],
      "id": "5VpnOFqhG7f0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Reflection (Deployment & Monitoring)\n",
        "Deploying this model in production comes with both opportunities and risks. The key risks include data drift, where the distribution of inputs changes over time, leading to degraded performance. Outliers or unexpected values in the data can also destabilize predictions, especially since we saw earlier that the model is sensitive to missing values and outlier treatment. At the system level, performance bottlenecks could appear if model inference time grows, impacting user experience. Finally, there are business risks if stakeholders interpret predictions without considering uncertainty, potentially leading to suboptimal decisions.\n",
        "To mitigate these risks, monitoring must occur across four layers.\n",
        "Data layer: track freshness of input data (e.g., data arriving late), null or missing rates, and schema consistency.\n",
        "Model layer: track rolling performance metrics such as MAE or AUC, calibration error, and feature importance stability. Alerts should trigger if error exceeds thresholds.\n",
        "System layer: monitor latency (e.g., p95 latency > 250ms), error rate, and uptime of the API or dashboard service.\n",
        "Business layer: track downstream KPIs such as approval rates, bad rates, or other decision-relevant metrics. This ensures the model is not only statistically accurate but also aligned with business goals.\n",
        "Ownership and handoffs must also be clear. The data engineering team owns data freshness and schema monitoring. The ML team owns model drift detection and retraining schedules. The platform or DevOps team owns latency and uptime. Business stakeholders own interpretation of metrics like approval rate. Clearly documenting these responsibilities and escalation paths ensures accountability.\n",
        "In summary, deployment is not just about serving predictions — it requires an end-to-end monitoring and governance plan. With proper monitoring thresholds, ownership handoffs, and retraining pipelines, the model can remain reliable, interpretable, and actionable in real-world use.\n"
      ],
      "metadata": {
        "id": "9-9D6kDWHLUO"
      },
      "id": "9-9D6kDWHLUO"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}